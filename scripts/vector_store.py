#!/usr/bin/env python3
"""
Notion í˜ì´ì§€ ë²¡í„° ì„ë² ë”© ë° Qdrant ì €ì¥
BGE-M3 (1024ì°¨ì› dense vector) + Qdrant
"""

import json
import os
import uuid
from pathlib import Path
from datetime import datetime
from dotenv import load_dotenv

from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct, PayloadSchemaType
from FlagEmbedding import BGEM3FlagModel
from tqdm import tqdm

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ê²½ë¡œ ì„¤ì •
DATA_DIR = Path(__file__).parent.parent / "data"
PAGES_FILE = DATA_DIR / "pages.json"

# Qdrant ì„¤ì •
QDRANT_HOST = os.environ.get("QDRANT_HOST", "localhost")
QDRANT_PORT = int(os.environ.get("QDRANT_PORT", 6333))
COLLECTION_NAME = os.environ.get("QDRANT_COLLECTION", "notion_pages")
VECTOR_DIM = int(os.environ.get("VECTOR_DIM", 1024))  # BGE-M3 dense ë²¡í„° ì°¨ì›

# ë°°ì¹˜ ì„¤ì •
BATCH_SIZE = 4  # CPUì—ì„œ ì•ˆì •ì ì¸ í¬ê¸°
MAX_TEXT_LENGTH = 2048  # ì†ë„ë¥¼ ìœ„í•´ ì œí•œ (ì›ë³¸ 8192)
UPSERT_BATCH_SIZE = 10  # Qdrant ì—…ì„œíŠ¸ ë¹ˆë„


def load_pages() -> list[dict]:
    """pages.json ë¡œë“œ"""
    print(f"Loading pages from {PAGES_FILE}...")
    with open(PAGES_FILE, "r", encoding="utf-8") as f:
        pages = json.load(f)
    print(f"Loaded {len(pages)} pages")
    return pages


def init_model() -> BGEM3FlagModel:
    """BGE-M3 ëª¨ë¸ ì´ˆê¸°í™”"""
    print("Loading BGE-M3 model...")
    model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True)
    print("Model loaded successfully")
    return model


def init_qdrant() -> QdrantClient:
    """Qdrant í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ë° ì»¬ë ‰ì…˜ ìƒì„±"""
    print(f"Connecting to Qdrant at {QDRANT_HOST}:{QDRANT_PORT}...")
    client = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT)

    # ê¸°ì¡´ ì»¬ë ‰ì…˜ í™•ì¸
    collections = [c.name for c in client.get_collections().collections]

    if COLLECTION_NAME in collections:
        print(f"Collection '{COLLECTION_NAME}' already exists. Recreating...")
        client.delete_collection(COLLECTION_NAME)

    # ì»¬ë ‰ì…˜ ìƒì„±
    client.create_collection(
        collection_name=COLLECTION_NAME,
        vectors_config=VectorParams(
            size=VECTOR_DIM,
            distance=Distance.COSINE
        )
    )

    # í˜ì´ë¡œë“œ ì¸ë±ìŠ¤ ìƒì„± (í•„í„°ë§ ì„±ëŠ¥ í–¥ìƒ)
    client.create_payload_index(
        collection_name=COLLECTION_NAME,
        field_name="word_count",
        field_schema=PayloadSchemaType.INTEGER
    )

    print(f"Collection '{COLLECTION_NAME}' created with {VECTOR_DIM}D vectors")
    return client


def prepare_text_for_embedding(page: dict) -> str:
    """ì„ë² ë”©ìš© í…ìŠ¤íŠ¸ ì¤€ë¹„ (ì œëª© + ë‚´ìš©)"""
    title = page.get("title", "")
    content = page.get("content", "")

    # ì œëª©ì„ ì•ì— ë¶™ì—¬ì„œ ê°€ì¤‘ì¹˜ ë¶€ì—¬
    text = f"{title}\n\n{content}" if content else title

    # ê¸¸ì´ ì œí•œ (ëŒ€ëµì ì¸ í† í° ì¶”ì •, í•œê¸€ì€ ~2 chars per token)
    max_chars = MAX_TEXT_LENGTH * 2
    if len(text) > max_chars:
        text = text[:max_chars]

    return text


def embed_batch(model: BGEM3FlagModel, texts: list[str]) -> list[list[float]]:
    """ë°°ì¹˜ ì„ë² ë”©"""
    result = model.encode(
        texts,
        batch_size=len(texts),
        max_length=MAX_TEXT_LENGTH,
        return_dense=True,
        return_sparse=False,
        return_colbert_vecs=False
    )
    return result['dense_vecs'].tolist()


def notion_id_to_uuid(notion_id: str) -> str:
    """Notion IDë¥¼ Qdrantìš© UUIDë¡œ ë³€í™˜"""
    clean_id = notion_id.replace("-", "")
    return str(uuid.UUID(clean_id))


def process_pages(
    pages: list[dict],
    model: BGEM3FlagModel,
    client: QdrantClient
) -> dict:
    """í˜ì´ì§€ ì„ë² ë”© ë° Qdrant ì €ì¥"""

    stats = {
        "total": len(pages),
        "processed": 0,
        "skipped_empty": 0,
        "errors": 0
    }

    # ì½˜í…ì¸ ê°€ ìˆëŠ” í˜ì´ì§€ë§Œ í•„í„°ë§
    valid_pages = []
    for page in pages:
        text = prepare_text_for_embedding(page)
        if text.strip():
            valid_pages.append((page, text))
        else:
            stats["skipped_empty"] += 1

    print(f"\nProcessing {len(valid_pages)} pages with content...")
    print(f"Skipped {stats['skipped_empty']} empty pages")

    # ë°°ì¹˜ ì²˜ë¦¬
    points = []

    for i in tqdm(range(0, len(valid_pages), BATCH_SIZE), desc="Embedding"):
        batch = valid_pages[i:i + BATCH_SIZE]
        batch_pages = [p[0] for p in batch]
        batch_texts = [p[1] for p in batch]

        try:
            # ì„ë² ë”© ìƒì„±
            vectors = embed_batch(model, batch_texts)

            # í¬ì¸íŠ¸ ìƒì„±
            for page, vector in zip(batch_pages, vectors):
                point = PointStruct(
                    id=notion_id_to_uuid(page["id"]),
                    vector=vector,
                    payload={
                        "notion_id": page["id"],
                        "title": page.get("title", ""),
                        "created_time": page.get("created_time", ""),
                        "last_edited_time": page.get("last_edited_time", ""),
                        "url": page.get("url", ""),
                        "word_count": page.get("word_count", 0),
                        "block_count": page.get("block_count", 0),
                        "parent_id": page.get("parent", {}).get("page_id", ""),
                        "content_preview": page.get("content", "")[:500],
                        "tags": page.get("tags", [])
                    }
                )
                points.append(point)
                stats["processed"] += 1

        except Exception as e:
            print(f"\nError processing batch: {e}")
            stats["errors"] += BATCH_SIZE
            continue

        # UPSERT_BATCH_SIZEê°œë§ˆë‹¤ Qdrantì— ì—…ì„œíŠ¸
        if len(points) >= UPSERT_BATCH_SIZE:
            try:
                client.upsert(collection_name=COLLECTION_NAME, points=points)
                points = []
            except Exception as e:
                print(f"\nQdrant upsert error: {e}")
                import time
                time.sleep(2)
                try:
                    client.upsert(collection_name=COLLECTION_NAME, points=points)
                    points = []
                except:
                    stats["errors"] += len(points)
                    points = []

    # ë‚¨ì€ í¬ì¸íŠ¸ ì—…ì„œíŠ¸
    if points:
        client.upsert(collection_name=COLLECTION_NAME, points=points)

    return stats


def test_semantic_search(client: QdrantClient, model: BGEM3FlagModel):
    """ì˜ë¯¸ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*60)
    print("Semantic Search Test")
    print("="*60)

    test_queries = [
        "í”„ë¡œì íŠ¸ ê´€ë¦¬ ë°©ë²•",
        "ì•„í‚¤í…ì²˜ ì„¤ê³„ íŒ¨í„´",
        "ê°œì¸ ëª©í‘œ ì„¤ì •",
    ]

    for query in test_queries:
        print(f"\nğŸ” Query: \"{query}\"")
        print("-" * 40)

        # ì¿¼ë¦¬ ì„ë² ë”©
        query_vector = model.encode(
            [query],
            max_length=512,
            return_dense=True,
            return_sparse=False,
            return_colbert_vecs=False
        )['dense_vecs'][0].tolist()

        # ê²€ìƒ‰
        results = client.query_points(
            collection_name=COLLECTION_NAME,
            query=query_vector,
            limit=3,
            with_payload=True
        )

        for i, hit in enumerate(results.points, 1):
            title = hit.payload.get("title", "Untitled")
            score = hit.score
            preview = hit.payload.get("content_preview", "")[:100]
            print(f"  {i}. [{score:.3f}] {title}")
            print(f"     {preview}...")


def main():
    start_time = datetime.now()
    print(f"Starting vector embedding at {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*60)

    # 1. ë°ì´í„° ë¡œë“œ
    pages = load_pages()

    # 2. ëª¨ë¸ ì´ˆê¸°í™”
    model = init_model()

    # 3. Qdrant ì´ˆê¸°í™”
    client = init_qdrant()

    # 4. ì„ë² ë”© ë° ì €ì¥
    stats = process_pages(pages, model, client)

    # 5. ê²°ê³¼ ì¶œë ¥
    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds()

    print("\n" + "="*60)
    print("Embedding Complete!")
    print("="*60)
    print(f"Total pages: {stats['total']}")
    print(f"Processed: {stats['processed']}")
    print(f"Skipped (empty): {stats['skipped_empty']}")
    print(f"Errors: {stats['errors']}")
    print(f"Duration: {duration:.1f} seconds ({duration/60:.1f} minutes)")

    # 6. ì»¬ë ‰ì…˜ ì •ë³´ í™•ì¸
    collection_info = client.get_collection(COLLECTION_NAME)
    print(f"\nQdrant Collection Info:")
    print(f"  Points count: {collection_info.points_count}")
    print(f"  Vector size: {collection_info.config.params.vectors.size}")

    # 7. ì˜ë¯¸ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    test_semantic_search(client, model)

    print("\nâœ… Vector embedding complete!")


if __name__ == "__main__":
    main()
